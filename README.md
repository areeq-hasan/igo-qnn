# IGO-QNN: Quantum Neural Network Architecture for Inductive Grover Oracularization

https://arxiv.org/abs/2105.11603

The inductive Grover oracular quantum neural network (IGO-QNN) is a novel paradigm of integration of Grover's unstructured search in a machine learning framework. The model defines a variational quantum circuit with hidden layers of parameterized quantum neurons densely connected via entangle synapses to encode a dynamic Grover's search oracle that can be trained from a set of database-hit training examples. This widens the range of problem applications of Grover's unstructured search algorithm to include the vast majority of problems lacking analytic descriptions of solution verifiers, allowing for quadratic speed-up in unstructured search for the set of search problems with relationships between input and output spaces that are tractably underivable deductively. This generalization of Grover's oracularization may prove particularly effective in deep reinforcement learning, computer vision, and, more generally, as a feature vector classifier at the top of an existing model.

## Model Architecture
### Quantum Neurons & Entangle Synapses
The inductive Grover's oracular quantum neural network (IGO-QNN) model we propose can be abstracted as a directed quantum graph of *quantum neural nodes* and *entangle synaptic edges*. Quantum neurons are encoded as a $U_3(\theta,\phi,\lambda)$ gate, where the gate and its parameters parallel classical neuron bias, a quantum nonlinear activation function such as radial Gaussian basis or Morelet wavelet basis functions encoded as a set of gates, and its outbound entangle-synapses, which connect it to other neurons. Entangle synapses are constituted of a trainable $U_3(\theta,\phi,\lambda)$ (distinct from the $U_3$ associated with a neural node) and a $CX$-gate which, when applied on qubits put in superposition via Hadamard gates, entangles two qubit channels by conditionally flipping the state of the target qubit depending on the state of the source qubit. The entangle synapse $U_3$-gate parameters serve as weights determining the extent to which two qubits are entangled, and can effectively allow for no entanglement of two qubits if nulled out ($L_1$ regularization can help fully null these weights). 

### Network Layers
The neural network consists of input, output, and hidden quantum layers, an oracle channel register, and a classical measure register. 

The input layer is a variational quantum register of $N$ qubit channels encoding unstructured database entries that is dynamically modified at each training iteration where a database-hit pair is passed as arguments to the network circuit. The output layer is a static quantum register of $N$ qubit channels encoding search hits and is the set of qubit channels are are measured and mapped to the classical output register of $N$ bits.

The $\ell$ hidden layers of the neural network are each constituted of $n_{\ell_i}$ neuron qubit channels. Each neuron in $\ell_i$ is densely connected to each neuron in $\ell_{i-1}$ for $1<i<\ell$ via an entangle synapse where neuron $n_i$ is the control neuron and $n_{i-1}$ is the target neuron. The gate abstracting this set of dense entangle synapse connections is referred to as the \textit{neural entangler}. The final hidden layer, $\ell_0$ is densely connected to each database entry in the output layer constituting the creation of the oracle in the output layer (oracle generator gate) that can then be applied to the input layer by another dense connection (oracularizer gate). Thus, the trainable neurons and synapses in the hidden layers conditionally applied to the output layer of the network effectively create a trainable Grover's unstructured oracle to be applied to $\ell_0$, where the number of hidden layers ($\ell$) and neurons per layer ($n_{\ell_i}$) encode the complexity of the search criteria.

### Network Propgation Behavior
The neural network is functionally encoded as a variational quantum circuit implementing a parameterized representation of Grover's search. The quantum neurons in each hidden layer and the oracle qubit are put in superposition via Hadamard transforms. Then the neural weights are initialized and applied to each neuron channel. Deeper hidden layer neurons first apply their respective activation functions and then are sequentially densely connected with each other via entangle synapses, and the final hidden layer is densely entangled with the output layer to create the oracle which is then applied to the input database. The output state of the neurally-encoded checker is then mapped to the oracle qubit (oracle flagging). Uncomputation using the inverse gate of the oracularizer is applied to the input and output layers, and the diffuser is applied for hit-state amplitude amplification. This sequence of oracularization, oracle flagging, uncomputation, and diffusion operations are iteratively applied $\sqrt N$ times (the optimal number of iterations for maximal amplitude amplification in Grover's algorithm). Finally, the oracle generation, neural entanglement, and weight initialization inverse gates are applied for uncomputation, and the output layer quantum register is measured to return the search hits for the oracle generated by the current model parameters. This network propagation behavior effectively allows for the neural network to search for hits in the input database using the trainable Grover's oracle. 

### Model Training
The inductive Grover's oracular neural network is trained via a classical optimizer. At each training iteration, the variational (parameterized) circuit is provided input arguments ($X$-gates applied to represent 1 in the input data), the network propagates, and the returned hits are compared to the expected hits. This comparison yields a value of a classical loss function, and the set of all model parameters constituted of the $\theta$, $\phi$, and $\lambda$ for each $U_3$ gate representing each quantum neuron and entangle synapse weight, are tweaked via the optimizer (such as Gradient Descent or Adam) to minimize the loss. The classical optimizer interfaces with an OpenQASM abstraction of the IGO-QNN class where the trainable params attribute is the dictionary of classical parameter components (floating-point), mutable by the optimizing agent. The classical optimizer iteratively evaluates the model training performance until the loss function appears to have been minimized, then returns a static quantum circuit with the optimal parameters fixed into the quantum neurons and entangle synapses | this is the ideal Grover's oracle encoding the ruleset for the trained search task, which can be continually tuned in a live training paradigm to reduce sensitivity to perturbations in the database distribution of hits. 

## Applications
The current paradigm for oracularization is the deductive representation of a decision problem (determining whether a proposed solution to a given problem is indeed a valid solution). As a consequence, an explicit solution to a given decision problem i.e. a solution-verification algorithm must be known in order to Grover's query a database for solutions to the associated search problem. This is the primary limitation of this deductive Grover's paradigm that narrows the range of applications to solely problems that contain closed form criteria for verification. However, of the set of all search problems, it is probability 1 that a given problem does not have a known deductive solution verifier. By means of the described quantum neural network architecture (IGO-QNN), we have described an inductive paradigm for the encoding of Grover's unstructured search oracles that allows for fast problem-space solution flagging without a closed form analytical framework for  verification. This widens the range of problem applications to include that vast majority without deductively-described oracles allowing for quadratic speed-up in unstructured search for problems with complex relationships between input and output spaces. The only prerequisite for the application of an IGO-QNN is that sufficient database-hit training examples constitute the training set for the classical optimizer to effectively minimize oracularization loss. 

This generalization of Grover's oracularization may prove particularly effective in deep reinforcement learning, computer vision, and, more generally, as a feature vector classifier at the top of an existing model. In the subdomain of reinforcement learning, a deep IGO-QNN can be given a database of potential future actions and the current state of the agent and flag the optimal decision. In the context of computer vision, an IGO-CNN can be given a database of images (likely semantically encoded into feature vectors) and be trained on matching image classification search hits to learn an oracle to flag images of a given classification. Any generalized set of feature vectors can be set as the input space of the network and the model can be trained to mark feature vectors that satisfy some condition for a general classification task. The networkâ€™s non-restrictive input/output space definition allows for it to be modularized and integrated as a layer in a larger machine learning model such as a hybrid quantum-classical context, allowing for the development of more robust models. 

## Usage

To create a new instance of a model,
```python
from igo_qnn import InductiveGroversQNN

model = InductiveGroversQNN(2, [{'size': 2, 'type': 'pairwise-full'}, {'size': 2, 'type': 'pairwise-full'}])
```
To cast the model to an instance of `qiskit_machine_learning.neural_networks.CircuitQNN`,
```python
qnn = model.toCircuitQNN()
```
